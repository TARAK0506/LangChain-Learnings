{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "groq_api_key = os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completion Model for Groq\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "LlamaModel = ChatGroq(\n",
    "    model = \"llama3-70b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt and Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a curious story about the legendary Sachin Tendulkar:\n",
      "\n",
      "**The Mysterious Batting Stance**\n",
      "\n",
      "In the early 2000s, Sachin Tendulkar, widely regarded as one of the greatest batsmen in cricket history, was going through a lean patch. Despite his incredible record, he was struggling to score runs, and his usually impeccable technique seemed to be letting him down.\n",
      "\n",
      "Desperate for a solution, Tendulkar turned to an unusual source: a Mumbai-based astrologer named Sushil Jain. Jain, who claimed to have helped several Indian cricketers with their game, was known for his unorthodox methods.\n",
      "\n",
      "According to reports, Jain told Tendulkar that the problem lay not with his technique, but with his batting stance. Specifically, the astrologer claimed that Tendulkar's stance was \"clashing with his stars.\" Yes, you read that right!\n",
      "\n",
      "Jain advised Tendulkar to change his batting stance to one that would align with his astrological signs. The Master Blaster, willing to try anything to regain his form, decided to give it a shot.\n",
      "\n",
      "For several weeks, Tendulkar worked on his new stance, which involved standing with his feet slightly wider apart and his bat held at a slightly different angle. It was a radical departure from his traditional style, but Tendulkar was determined to make it work.\n",
      "\n",
      "And then, something strange happened. Tendulkar started scoring runs again – plenty of them. He hit centuries and half-centuries with ease, and his form began to resemble the Tendulkar of old.\n",
      "\n",
      "While it's impossible to say whether the astrologer's advice was the sole reason for Tendulkar's resurgence, it's undeniable that the change in stance had a significant impact on his game. Some even joked that Jain had become Tendulkar's \"lucky charm\"!\n",
      "\n",
      "This curious episode serves as a reminder that, even in the world of sports, sometimes the most unlikely solutions can lead to success. And who knows? Maybe there's some truth to the idea that the stars do influence our lives – even on the cricket pitch!\n"
     ]
    }
   ],
   "source": [
    "# Completetion model for Groq\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} story about {story}.\"\n",
    ")\n",
    "\n",
    "llmModelPrompt = prompt_template.format(\n",
    "    adjective = \"curious\",\n",
    "    story = \"Sachin Tendulkar\"\n",
    ")\n",
    "\n",
    "res = LlamaModel.invoke(llmModelPrompt)\n",
    "\n",
    "print(res.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for Large Language Model. It's a type of artificial intelligence (AI) model that's specifically designed to process and generate human-like language.\n",
      "\n",
      "LLMs are typically trained on massive amounts of text data, such as books, articles, and websites, which enables them to learn patterns, relationships, and structures of language. This training allows LLMs to generate coherent, context-specific, and often surprisingly accurate text.\n",
      "\n",
      "Some key characteristics of LLMs include:\n",
      "\n",
      "1. **Scalability**: LLMs are trained on vast amounts of data, making them capable of handling complex language tasks.\n",
      "2. **Generative capabilities**: LLMs can generate text that's often indistinguishable from human-written content.\n",
      "3. **Contextual understanding**: LLMs can understand the context in which language is being used, allowing them to respond appropriately.\n",
      "4. **Improvisation**: LLMs can improvise and create novel text based on the input they receive.\n",
      "\n",
      "LLMs have many applications, such as:\n",
      "\n",
      "1. **Chatbots**: LLMs can be used to generate human-like responses in chatbots, making them more conversational and engaging.\n",
      "2. **Content generation**: LLMs can be used to generate content, such as articles, social media posts, and even entire books.\n",
      "3. **Language translation**: LLMs can be fine-tuned for language translation tasks, allowing for more accurate and natural-sounding translations.\n",
      "4. **Text summarization**: LLMs can summarize long pieces of text into concise, meaningful summaries.\n",
      "\n",
      "Some examples of LLMs include transformer-based models like BERT, RoBERTa, and XLNet, as well as more recent models like DALL-E and GLaM.\n",
      "\n",
      "Do you have any specific questions about LLMs or their applications?\n"
     ]
    }
   ],
   "source": [
    "# Chat Completeion Model for Groq\n",
    "# Few Shot Prompt\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an {Profession} expert on {Topic}.\"),\n",
    "        (\"human\",\"Hello, Mr.{Profession}, can you please answer my question on {Topic}?\"),\n",
    "        (\"ai\", \"Sure, I can help you with that. What is your question?\"),\n",
    "        (\"human\", \"I want to know about {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    Profession = \"AI\",\n",
    "    Topic = \"Generative AI\",\n",
    "    question = \"What is LLM?\"\n",
    ")\n",
    "\n",
    "response = LlamaModel.invoke(messages)\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! I'm delighted to help. As a renowned Neuroscientist expert on Memory and Learning, I'd be more than happy to dive into your question on the fascinating realm of human cognition. Please go ahead and ask away, and I'll do my best to provide you with a detailed and insightful response!\n"
     ]
    }
   ],
   "source": [
    "# old way\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content =(\n",
    "                \"You are an {Profession} expert on {Topic}.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    user_input = \"Hello, Mr.{Profession}, can you please answer my question on {Topic}?\",\n",
    ")\n",
    "\n",
    "response = LlamaModel.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish Translation\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\":\"hi\", \"output\":\"hola\"},\n",
    "    {\"input\":\"how are you\", \"output\":\"como estas\"},\n",
    "    {\"input\":\"good morning\", \"output\":\"buenos dias\"},\n",
    "    {\"input\":\"good night\", \"output\":\"buenas noches\"},\n",
    "    {\"input\":\"goodbye\", \"output\":\"adios\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\" , \"{input}\"),\n",
    "        (\"ai\" , \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert in English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\" , \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cómo estás?\n",
      "\n",
      "(Translation: I'm fine, thank you. - Estoy bien, gracias.)\n"
     ]
    }
   ],
   "source": [
    "# Spanish Translation\n",
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\":\"hi\", \"output\":\"hola\"},\n",
    "    {\"input\":\"goodbye\", \"output\":\"adios\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\" , \"{input}\"),\n",
    "        (\"ai\" , \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_prompt,\n",
    "    examples = examples\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert in English-Spanish translator.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\" , \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = final_prompt | LlamaModel\n",
    "\n",
    "response = chain.invoke({\"input\":\"How are you?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "LlamaModel = ChatGroq(\n",
    "    model = \"llama3-70b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'New Delhi'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with an 'answer' key that answers the following question {question} :\"\n",
    ")\n",
    "\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "\n",
    "chain = json_prompt | LlamaModel | json_parser\n",
    "\n",
    "response = chain.invoke({\"question\":\"What is the capital of India?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
